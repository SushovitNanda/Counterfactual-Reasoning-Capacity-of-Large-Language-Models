{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Installing the Necessary modules and libraries**"
      ],
      "metadata": {
        "id": "1WLVgBe58UsM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLXwJqbjtPho"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n",
        "!pip install rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import os\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer\n",
        "import warnings\n",
        "from rouge_score import rouge_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}"
      ],
      "metadata": {
        "id": "nAMzy_0FtaUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#put your own huggingface token after gaining gated model access\n",
        "#select \"n\"\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "Lj62GWj6P-s_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*https://huggingface.co/models*"
      ],
      "metadata": {
        "id": "wlL0Prah8zSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The model that you want to train from the Hugging Face hub\n",
        "model_name = \"meta-llama/Llama-3-8b-Instruct\"\n",
        "# Fine-tuned model name\n",
        "new_model = \"Llama-3-8b-Instruct-finetune\""
      ],
      "metadata": {
        "id": "ib_We3NLtj2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "df = pd.read_json(\"test.json\")\n",
        "df.to_csv(\"test.csv\")\n",
        "df = pd.read_csv('test.csv')\n",
        "df = df.head(100)\n",
        "\n",
        "#Format the dataset as per the model requirements.\n",
        "df['text'] = '<S>' + \"[INST]\" + df['question'] + df['context'] + '[/INST]' + df['answers'] + '</S>'\n",
        "# Remove inplace=True to return a new DataFrame\n",
        "dtf = df.drop(columns=['question','answers','context', 'idx','Unnamed: 0'], axis=1)\n",
        "\n",
        "#convert to Huggingface Datasets format\n",
        "train = Dataset.from_pandas(dtf)\n",
        "dataset = train # Use the Hugging Face Dataset object here"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "T1HIts3QnFxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_json(\"dev.json\")\n",
        "test.to_csv(\"dev.csv\")\n",
        "test = pd.read_csv('dev.csv')\n",
        "test['text'] = test['question'] + test['context']\n",
        "#test.drop(columns=['question','answers','context', 'idx','Unnamed: 0'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "xtUKpmRcFrHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, \"float16\")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit= True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_use_double_quant= False,\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and True:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Load LLaMA tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
        "\n",
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha= 16,\n",
        "    lora_dropout= 0.1,\n",
        "    r= 64,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs= 1,  #4\n",
        "    per_device_train_batch_size= 4,\n",
        "    per_device_eval_batch_size = 4,\n",
        "    gradient_accumulation_steps= 1,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps= 0,\n",
        "    logging_steps= 25,\n",
        "    learning_rate= 2e-4,\n",
        "    weight_decay= 0.001,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    max_grad_norm= 0.3,\n",
        "    max_steps= -1,\n",
        "    warmup_ratio= 0.03,\n",
        "    group_by_length= None,\n",
        "    lr_scheduler_type= \"cosine\",\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "OJXpOgBFuSrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset= test,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length= None,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing= False,\n",
        ")"
      ],
      "metadata": {
        "id": "xN_m-f5NEOSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "P0twHhrKERrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save trained model\n",
        "trainer.model.save_pretrained(new_model)"
      ],
      "metadata": {
        "id": "aQNaFYp40M6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Run text generation pipeline with our next model\n",
        "ques = test['text'].iloc[9]\n",
        "prompt = ques\n",
        "task = 'You are a helpful, respectful and honest assistant.  Always give factually coherent answers. Keep your answers to be brief within 3 sentences. Do not refuse to give an answer'\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\n",
        "result = pipe(f\"<s> {task} [INST] {prompt} [/INST]\")\n",
        "generated_text = result[0]['generated_text']"
      ],
      "metadata": {
        "id": "frlSLPin4IJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text = generated_text.split('[/INST]')[-1].strip()\n",
        "print(clean_text)"
      ],
      "metadata": {
        "id": "6Me-g1K5KgRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Empty VRAM\n",
        "del model\n",
        "del pipe\n",
        "del trainer\n",
        "import gc\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "mkQCviG0Zta-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload model in FP16 and merge it with LoRA weights\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, new_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "QQn30cRtAZ-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ques = test['text'].iloc[99]\n",
        "prompt = ques\n",
        "task = 'You are a helpful, respectful and honest assistant.  Always give factually coherent answers. Keep your answers to be brief within 3 sentences. Do not refuse to give an answer'\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\n",
        "result = pipe(f\"<s> {task} [INST] {prompt} [/INST]\")\n",
        "generated_text = result[0]['generated_text']"
      ],
      "metadata": {
        "id": "UwfgUuy_ppsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text = generated_text.split('[/INST]')[-1].strip()\n",
        "print(clean_text)"
      ],
      "metadata": {
        "id": "--oDiE2dMWeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = df.head(10)"
      ],
      "metadata": {
        "id": "wTlt8r-vhTMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\n",
        "rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Define the task\n",
        "task = 'You are a helpful, respectful and honest assistant. Always give factually coherent answers. Keep your answers brief within 3 sentences. Do not refuse to give an answer.'\n",
        "\n",
        "# Initialize lists to store results\n",
        "all_rouge_scores = []\n",
        "\n",
        "# Iterate through the dataset\n",
        "for idx in range(len(test)):\n",
        "    ques = test['text'].iloc[idx]\n",
        "    ans = test['answers'].iloc[idx]\n",
        "\n",
        "    prompt = ques\n",
        "    result = pipe(f\"<s> {task} [INST] {prompt} [/INST]\")\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    clean_text = generated_text.split('[/INST]')[-1].strip()\n",
        "    print(f\"Generated Text for question {idx+1}:\", clean_text)\n",
        "    print(f\"True Answer for question {idx+1}:\", ans)\n",
        "\n",
        "    # Calculate ROUGE scores\n",
        "    rouge_scores = []\n",
        "    for true_answer_list in ans:\n",
        "        true_answer = ' '.join(true_answer_list)\n",
        "        scores = rouge_scorer_instance.score(true_answer, clean_text)\n",
        "        rouge_scores.append(scores)\n",
        "\n",
        "    all_rouge_scores.append(rouge_scores)\n",
        "\n",
        "# Calculate and print average ROUGE scores\n",
        "avg_rouge1 = sum(score['rouge1'].fmeasure for scores in all_rouge_scores for score in scores) / sum(len(scores) for scores in all_rouge_scores)\n",
        "avg_rouge2 = sum(score['rouge2'].fmeasure for scores in all_rouge_scores for score in scores) / sum(len(scores) for scores in all_rouge_scores)\n",
        "avg_rougeL = sum(score['rougeL'].fmeasure for scores in all_rouge_scores for score in scores) / sum(len(scores) for scores in all_rouge_scores)\n",
        "\n",
        "print(f'Average ROUGE-1 F1 Score: {avg_rouge1}')\n",
        "print(f'Average ROUGE-2 F1 Score: {avg_rouge2}')\n",
        "print(f'Average ROUGE-L F1 Score: {avg_rougeL}')"
      ],
      "metadata": {
        "id": "vSItJHUjfat1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aGtjC5FhnnOH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}