{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlBAcQ56zc_J"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**All Required Libraries and installations needed**"
      ],
      "metadata": {
        "id": "ik6rDQyv3d67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose\n",
        "!pip install huggingface_hub\n",
        "!pip install llama-cpp-python==0.1.78\n",
        "!pip install numpy==1.23.4"
      ],
      "metadata": {
        "id": "oNVCW9KXzjU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "from sklearn.metrics import f1_score\n",
        "!pip install rouge-score\n",
        "from rouge_score import rouge_scorer\n",
        "data = pd.read_json(\"test.json\")"
      ],
      "metadata": {
        "id": "eGRSPqxDzjJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **The quantized Llama2-13b-Chat model**\n",
        "\n",
        " https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF\n",
        "\n",
        " can use other finetuned models available in the repo too\n"
      ],
      "metadata": {
        "id": "FLHFTWS_3s5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\""
      ],
      "metadata": {
        "id": "0zjdEkJEzjSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
        "# GPU\n",
        "lcpp_llm = None\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=2,\n",
        "    n_batch=512,\n",
        "    n_gpu_layers=32)"
      ],
      "metadata": {
        "id": "Lw663J2HzjMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Testing the Model**"
      ],
      "metadata": {
        "id": "0VmGW-3Z4Knn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = data_test.iloc[0]\n",
        "prompt = data1\n",
        "prompt_template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer in very brief with simple 2 lines of explanation.\n",
        "\n",
        "USER: {prompt}\n",
        "\n",
        "ASSISTANT:\n",
        "'''"
      ],
      "metadata": {
        "id": "zsYQ4keN4JrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=lcpp_llm(prompt=prompt_template, max_tokens=256, temperature=0.5, top_p=0.95,\n",
        "                  repeat_penalty=1.2, top_k=150,\n",
        "                  echo=False)\n",
        "print(response[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "id": "-W2FuXg04JYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_sample = data.head(30)\n",
        "\n",
        "model_responses = []\n",
        "for question in data_sample['question']:\n",
        "    prompt_template = f'''SYSTEM: You are a helpful, respectful and honest assistant. Always give factually coherent answers.\n",
        "\n",
        "USER: {question}\n",
        "\n",
        "ASSISTANT:\n",
        "'''\n",
        "    response = lcpp_llm(prompt=prompt_template, max_tokens=256, temperature=0, top_p=0.95,\n",
        "                        repeat_penalty=1.2, top_k=150, echo=False)\n",
        "    model_responses.append(response[\"choices\"][0][\"text\"])\n",
        "\n",
        "data_sample['model_response'] = model_responses"
      ],
      "metadata": {
        "id": "SSLZk4nLzjCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applying ROUGE SCORE to evaluate**"
      ],
      "metadata": {
        "id": "8ggaS4w64_76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply ROUGE scores\n",
        "rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge_scores = []\n",
        "for true_answer_list, model_answer in zip(data_sample['answers'], data_sample['model_response']):\n",
        "    # Assuming 'true_answer_list' is a list of strings, join them into a single string\n",
        "    true_answer = ' '.join(true_answer_list)\n",
        "    scores = rouge_scorer_instance.score(true_answer, model_answer)\n",
        "    rouge_scores.append(scores)\n",
        "\n",
        "data_sample['rouge_score'] = rouge_scores"
      ],
      "metadata": {
        "id": "kgY-2_1Mzi8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average scores\n",
        "avg_rouge1 = sum(score['rouge1'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "avg_rouge2 = sum(score['rouge2'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "avg_rougeL = sum(score['rougeL'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "\n",
        "print(f'Average ROUGE-1 F1 Score: {avg_rouge1}')\n",
        "print(f'Average ROUGE-2 F1 Score: {avg_rouge2}')\n",
        "print(f'Average ROUGE-L F1 Score: {avg_rougeL}')"
      ],
      "metadata": {
        "id": "mgCyQaDu0t57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply ROUGE scores\n",
        "rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge_scores = []\n",
        "for true_answer_list, model_answer in zip(data_sample['context'], data_sample['model_response']):\n",
        "    # Assuming 'true_answer_list' is a list of strings, join them into a single string\n",
        "    true_answer = ' '.join(true_answer_list)\n",
        "    scores = rouge_scorer_instance.score(true_answer, model_answer)\n",
        "    rouge_scores.append(scores)\n",
        "\n",
        "data_sample['rouge_score'] = rouge_scores\n"
      ],
      "metadata": {
        "id": "98dltDX10t3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average scores\n",
        "avg_rouge1 = sum(score['rouge1'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "avg_rouge2 = sum(score['rouge2'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "avg_rougeL = sum(score['rougeL'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "\n",
        "print(f'Average ROUGE-1 F1 Score: {avg_rouge1}')\n",
        "print(f'Average ROUGE-2 F1 Score: {avg_rouge2}')\n",
        "print(f'Average ROUGE-L F1 Score: {avg_rougeL}')"
      ],
      "metadata": {
        "id": "XB7pr-cU0tyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print sample results\n",
        "for index, row in data_sample.iterrows():\n",
        "    print(f\"Question {index+1}:\")\n",
        "    print(f\"Question: {row['question']}\")\n",
        "    print(f\"Context: {row['context']}\")\n",
        "    print(f\"True Answer: {row['answers']}\")\n",
        "    print(f\"Model Response: {row['model_response']}\")\n",
        "    print(f\"ROUGE Scores: {row['rouge_score']}\")\n"
      ],
      "metadata": {
        "id": "Sk7zSe6A0t1D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}