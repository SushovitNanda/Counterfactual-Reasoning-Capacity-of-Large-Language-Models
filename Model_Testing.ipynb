{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Installation of necessary modules and libraries**\n",
        "\n",
        "Upload a new txt file and paste the following modules\n",
        "\n",
        "accelerate == 0.31.0\n",
        "\n",
        "bitsandbytes == 0.43.1\n",
        "\n",
        "transformers == 4.42.3"
      ],
      "metadata": {
        "id": "_bsawQP_6R_j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gjCfRgqv1qW"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import (AutoTokenizer,\n",
        "                          AutoModelForCausalLM,\n",
        "                          BitsAndBytesConfig,\n",
        "                          pipeline)\n",
        "from sklearn.metrics import f1_score\n",
        "!pip install rouge-score\n",
        "from rouge_score import rouge_scorer\n",
        "#Import the dataset\n",
        "data = pd.read_json(\"test.json\")"
      ],
      "metadata": {
        "id": "Zr6WpLKL1lw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get your own access to the gated LLM models from huggingface or from the META website**"
      ],
      "metadata": {
        "id": "u1EQuYQf624M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HF_TOKEN = #paste your own huggingface token"
      ],
      "metadata": {
        "id": "I7JrTEFK2qFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#select the model you wish to work on\n",
        "model_name = \"meta-llama/Meta-Llama-3-8B\""
      ],
      "metadata": {
        "id": "vJz0CGbH3Srp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting up the quantization params\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "sISJd0jZ3gZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
        "                                          token=HF_TOKEN)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "SsgTkXre4exI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    token=HF_TOKEN\n",
        ")"
      ],
      "metadata": {
        "id": "VCL_Ouco4-kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the pipeline documentation from hugging face to learn about the parameters\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.01,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.15,\n",
        "    top_k = 1,\n",
        "    #echo = False\n",
        ")"
      ],
      "metadata": {
        "id": "wXVA_JMw55FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test1 = data.iloc[0]\n",
        "\n",
        "prompt_template = f'''SYSTEM: You are a helpful, respectful and honest assistant. Always give factually coherent answers. Keep your answers to be brief within 3 sentences.\n",
        "\n",
        "USER: {test1['question']}\n",
        "\n",
        "CONTEXT: {test1['context']}\n",
        "\n",
        "ASSISTANT:\n",
        "'''\n",
        "response = generator(prompt_template)"
      ],
      "metadata": {
        "id": "nD4Kukim9Iw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = response[0][\"generated_text\"]\n",
        "if \"ASSISTANT:\" in generated_text:\n",
        "    assistant_response = generated_text.split(\"ASSISTANT:\")[1].strip().split(\"\\n\")[0].strip()\n",
        "else:\n",
        "    assistant_response = generated_text.strip()\n",
        "\n",
        "print(assistant_response)"
      ],
      "metadata": {
        "id": "vBTjfvNk9IqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test2 = data.iloc[5]\n",
        "prompt_template = f'''SYSTEM: You are a helpful, respectful and honest assistant. Interpret the context given carefully.  Always give factually coherent answers. Always give an answer.\n",
        "Try to be mathematically correct.\n",
        "USER: {test2['question']}\n",
        "\n",
        "CONTEXT: {test2['context']}\n",
        "\n",
        "ASSISTANT:\n",
        "'''\n",
        "response = generator(prompt_template)"
      ],
      "metadata": {
        "id": "JaEwXd-F9InU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = response[0][\"generated_text\"]\n",
        "if \"ASSISTANT:\" in generated_text:\n",
        "    assistant_response = generated_text.split(\"ASSISTANT:\")[1].split(\"USER:\")[0].strip().replace('\\n', ' ').strip().split(\"##\")[0].strip().replace('\\n', ' ')\n",
        "else:\n",
        "    assistant_response = generated_text.strip().replace('\\n', ' ')\n",
        "\n",
        "assistant_response"
      ],
      "metadata": {
        "id": "_f3d_oOS9Ik1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_sample = data.head(30)\n",
        "\n",
        "model_responses = []\n",
        "for index, row in data_sample.iterrows():\n",
        "    prompt_template = f'''SYSTEM: You are a helpful, respectful and honest assistant. Interpret the context given carefully.  Always give factually coherent answers. Always give an answer.\n",
        "\n",
        "USER: {row['question']}\n",
        "\n",
        "CONTEXT: {row['context']}\n",
        "\n",
        "ASSISTANT:\n",
        "'''\n",
        "    response = generator(prompt_template)\n",
        "    generated_text = response[0][\"generated_text\"]\n",
        "    if \"ASSISTANT:\" in generated_text:\n",
        "        assistant_response = generated_text.split(\"ASSISTANT:\")[1].split(\"USER:\")[0].strip().replace('\\n', ' ').strip().split(\"##\")[0].strip().replace('\\n', ' ').strip().split(\"SYSTEM:\")[0].strip().replace('\\n', ' ')\n",
        "    else:\n",
        "        assistant_response = generated_text.strip().replace('\\n', ' ')\n",
        "    model_responses.append(assistant_response)\n",
        "\n",
        "data_sample['model_response'] = model_responses"
      ],
      "metadata": {
        "id": "ZsyhdectFyG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply ROUGE scores\n",
        "rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge_scores = []\n",
        "for true_answer_list, model_answer in zip(data_sample['answers'], data_sample['model_response']):\n",
        "    # Assuming 'true_answer_list' is a list of strings, join them into a single string\n",
        "    true_answer = ' '.join(true_answer_list)\n",
        "    scores = rouge_scorer_instance.score(true_answer, model_answer)\n",
        "    rouge_scores.append(scores)\n",
        "\n",
        "data_sample['rouge_score'] = rouge_scores\n"
      ],
      "metadata": {
        "id": "EVe0Sw-zHGb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average scores\n",
        "avg_rouge1 = sum(score['rouge1'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "avg_rouge2 = sum(score['rouge2'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "avg_rougeL = sum(score['rougeL'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "\n",
        "print(f'Average ROUGE-1 F1 Score: {avg_rouge1}')\n",
        "print(f'Average ROUGE-2 F1 Score: {avg_rouge2}')\n",
        "print(f'Average ROUGE-L F1 Score: {avg_rougeL}')"
      ],
      "metadata": {
        "id": "Bfbm9A0bHGZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply ROUGE scores\n",
        "rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge_scores = []\n",
        "for true_answer_list, model_answer in zip(data_sample['context'], data_sample['model_response']):\n",
        "    # Assuming 'true_answer_list' is a list of strings, join them into a single string\n",
        "    true_answer = ' '.join(true_answer_list)\n",
        "    scores = rouge_scorer_instance.score(true_answer, model_answer)\n",
        "    rouge_scores.append(scores)\n",
        "\n",
        "data_sample['rouge_score'] = rouge_scores\n"
      ],
      "metadata": {
        "id": "dzirL4jXHGWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average scores\n",
        "avg_rouge1 = sum(score['rouge1'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "avg_rouge2 = sum(score['rouge2'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "avg_rougeL = sum(score['rougeL'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "\n",
        "print(f'Average ROUGE-1 F1 Score: {avg_rouge1}')\n",
        "print(f'Average ROUGE-2 F1 Score: {avg_rouge2}')\n",
        "print(f'Average ROUGE-L F1 Score: {avg_rougeL}')"
      ],
      "metadata": {
        "id": "zeQOjveAHGUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in data_sample.iterrows():\n",
        "    print(f\"Question: {index+1}:\")\n",
        "    print(f\"Question: {row['question']}\")\n",
        "    print(f\"Context: {row['context']}\")\n",
        "    print(f\"Model Response: {row['model_response']}\")\n",
        "    print(f\"True Answers: {row['answers']}\")\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oergD3_BHGRY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}