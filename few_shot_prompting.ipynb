{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install the required modules and libraries**\n",
        "\n",
        "upload a txt file and paste these modules in it\n",
        "\n",
        "accelerate == 0.31.0\n",
        "\n",
        "bitsandbytes == 0.43.1\n",
        "\n",
        "transformers == 4.42.3"
      ],
      "metadata": {
        "id": "xywGVgsS9w-M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gjCfRgqv1qW"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt  # accelerate == 0.31.0\n",
        "                                  #bitsandbytes == 0.43.1\n",
        "                                  #transformers == 4.42.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import (AutoTokenizer,\n",
        "                          AutoModelForCausalLM,\n",
        "                          BitsAndBytesConfig,\n",
        "                          pipeline)\n",
        "from sklearn.metrics import f1_score\n",
        "!pip install rouge-score\n",
        "from rouge_score import rouge_scorer\n",
        "data = pd.read_json(\"test.json\")"
      ],
      "metadata": {
        "id": "Zr6WpLKL1lw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HF_TOKEN = #paste your own Huggingface token in here after getting the gated model access"
      ],
      "metadata": {
        "id": "I7JrTEFK2qFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*https://huggingface.co/models*"
      ],
      "metadata": {
        "id": "-g0S42Qn-IQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\""
      ],
      "metadata": {
        "id": "vJz0CGbH3Srp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting quantization parameters\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "sISJd0jZ3gZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
        "                                          token=HF_TOKEN)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "SsgTkXre4exI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    token=HF_TOKEN\n",
        ")"
      ],
      "metadata": {
        "id": "VCL_Ouco4-kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.01,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.15,\n",
        "    top_k = 1,\n",
        "    #echo = False\n",
        ")"
      ],
      "metadata": {
        "id": "wXVA_JMw55FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = data.iloc[0]\n",
        "\n",
        "prompt_template = f'''SYSTEM: You are a helpful, respectful and honest assistant. Always give factually coherent answers. Keep your answers to be brief within 3 sentences.\n",
        "\n",
        "USER: {data1['question']}\n",
        "\n",
        "CONTEXT: {data1['context']}\n",
        "\n",
        "ASSISTANT:\n",
        "'''\n",
        "response = generator(prompt_template)"
      ],
      "metadata": {
        "id": "nD4Kukim9Iw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = response[0][\"generated_text\"]\n",
        "if \"ASSISTANT:\" in generated_text:\n",
        "    assistant_response = generated_text.split(\"ASSISTANT:\")[1].strip().split(\"\\n\")[0].strip()\n",
        "else:\n",
        "    assistant_response = generated_text.strip()\n",
        "\n",
        "print(assistant_response)"
      ],
      "metadata": {
        "id": "vBTjfvNk9IqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = data.iloc[5]\n",
        "data8 = data.iloc[8]\n",
        "prompt_template = f'''SYSTEM: You are a helpful, respectful and honest assistant. Interpret the context given carefully.  Always give factually coherent answers. Always give an answer.\n",
        "Try to be mathematically correct.\n",
        "USER: {data2['question']}\n",
        "\n",
        "CONTEXT: {data2['context']}\n",
        "\n",
        "ASSISTANT: {data2['answers']}\n",
        "\n",
        "User: {data8['question']}\n",
        "\n",
        "Context: {data8['context']}\n",
        "\n",
        "Assistant:\n",
        "'''\n",
        "response = generator(prompt_template)"
      ],
      "metadata": {
        "id": "JaEwXd-F9InU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = response[0][\"generated_text\"]\n",
        "if \"ASSISTANT:\" in generated_text:\n",
        "    assistant_response = generated_text.split(\"Assistant:\")[1].split(\"USER:\")[0].strip().replace('\\n', ' ').strip().split(\"##\")[0].strip().replace('\\n', ' ')\n",
        "else:\n",
        "    assistant_response = generated_text.strip().replace('\\n', ' ')\n",
        "\n",
        "assistant_response"
      ],
      "metadata": {
        "id": "_f3d_oOS9Ik1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Define the few-shot examples\n",
        "#picking random examples from the dataset\n",
        "prom = data.iloc[100]\n",
        "promp = data.iloc[101]\n",
        "promt = data.iloc[102]\n",
        "few_shot_examples = [\n",
        "    {\n",
        "        \"question\": prom['question'],\n",
        "        \"context\": prom['context'],\n",
        "        \"answer\": prom['answers']\n",
        "    },\n",
        "    {\n",
        "        \"question\": promp['question'],\n",
        "        \"context\": promp['context'],\n",
        "        \"answer\": promp['answers']\n",
        "    },\n",
        "    {\n",
        "        \"question\": promt['question'],\n",
        "        \"context\": promt['context'],\n",
        "        \"answer\": promt['answers']\n",
        "    },\n",
        "]"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mGHuX1v-8Iiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the new question and context\n",
        "data9 = data.iloc[9]\n",
        "new_question = data9['question']\n",
        "context = data9['context']"
      ],
      "metadata": {
        "id": "ZA_Brf_E7NzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct the input prompt\n",
        "def construct_prompt(examples, question, context):\n",
        "    prompt = \"Answer the following questions based on the context provided.\\n\\n\"\n",
        "    for example in examples:\n",
        "        prompt += f\"Question: {example['question']}\\nAnswer: {example['answer']}\\n\\n\"\n",
        "    prompt += f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
        "    return prompt\n",
        "prompt = construct_prompt(few_shot_examples, new_question, context)"
      ],
      "metadata": {
        "id": "MQNiGnz87NtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the answer\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\n",
        "result = pipe(prompt)\n",
        "\n",
        "# Extract and print the generated answer\n",
        "generated_text = result[0]['generated_text']\n",
        "clean_text = generated_text.split('Answer:')[-1].strip()"
      ],
      "metadata": {
        "id": "ZJGnPiTt8m4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text"
      ],
      "metadata": {
        "id": "CLoqZy8z8m1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_sample = data.head(3)\n",
        "\n",
        "model_responses = []\n",
        "for index, row in data_sample.iterrows():\n",
        "    prompt_template = f'''SYSTEM: You are a helpful, respectful and honest assistant. Interpret the context given carefully.  Always give factually coherent answers. Always give an answer.\n",
        "\n",
        "USER: {row['question']}\n",
        "\n",
        "CONTEXT: {row['context']}\n",
        "\n",
        "ASSISTANT:\n",
        "'''\n",
        "    response = generator(prompt_template)\n",
        "    generated_text = response[0][\"generated_text\"]\n",
        "    if \"ASSISTANT:\" in generated_text:\n",
        "        assistant_response = generated_text.split(\"ASSISTANT:\")[1].split(\"USER:\")[0].strip().replace('\\n', ' ').strip().split(\"##\")[0].strip().replace('\\n', ' ').strip().split(\"SYSTEM:\")[0].strip().replace('\\n', ' ')\n",
        "    else:\n",
        "        assistant_response = generated_text.strip().replace('\\n', ' ')\n",
        "    model_responses.append(assistant_response)\n",
        "\n",
        "data_sample['model_response'] = model_responses"
      ],
      "metadata": {
        "id": "ZsyhdectFyG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply ROUGE scores\n",
        "rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge_scores = []\n",
        "for true_answer_list, model_answer in zip(data_sample['answers'], data_sample['model_response']):\n",
        "    # Assuming 'true_answer_list' is a list of strings, join them into a single string\n",
        "    true_answer = ' '.join(true_answer_list)\n",
        "    scores = rouge_scorer_instance.score(true_answer, model_answer)\n",
        "    rouge_scores.append(scores)\n",
        "\n",
        "data_sample['rouge_score'] = rouge_scores\n"
      ],
      "metadata": {
        "id": "EVe0Sw-zHGb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average scores\n",
        "avg_rouge1 = sum(score['rouge1'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "avg_rouge2 = sum(score['rouge2'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "avg_rougeL = sum(score['rougeL'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "\n",
        "print(f'Average ROUGE-1 F1 Score: {avg_rouge1}')\n",
        "print(f'Average ROUGE-2 F1 Score: {avg_rouge2}')\n",
        "print(f'Average ROUGE-L F1 Score: {avg_rougeL}')"
      ],
      "metadata": {
        "id": "Bfbm9A0bHGZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply ROUGE scores\n",
        "rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge_scores = []\n",
        "for true_answer_list, model_answer in zip(data_sample['context'], data_sample['model_response']):\n",
        "    # Assuming 'true_answer_list' is a list of strings, join them into a single string\n",
        "    true_answer = ' '.join(true_answer_list)\n",
        "    scores = rouge_scorer_instance.score(true_answer, model_answer)\n",
        "    rouge_scores.append(scores)\n",
        "\n",
        "data_sample['rouge_score'] = rouge_scores"
      ],
      "metadata": {
        "id": "dzirL4jXHGWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average scores\n",
        "avg_rouge1 = sum(score['rouge1'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "avg_rouge2 = sum(score['rouge2'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "avg_rougeL = sum(score['rougeL'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "\n",
        "print(f'Average ROUGE-1 F1 Score: {avg_rouge1}')\n",
        "print(f'Average ROUGE-2 F1 Score: {avg_rouge2}')\n",
        "print(f'Average ROUGE-L F1 Score: {avg_rougeL}')"
      ],
      "metadata": {
        "id": "zeQOjveAHGUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in data_sample.iterrows():\n",
        "    print(f\"Question: {index+1}:\")\n",
        "    print(f\"Question: {row['question']}\")\n",
        "    print(f\"Context: {row['context']}\")\n",
        "    print(f\"Model Response: {row['model_response']}\")\n",
        "    print(f\"True Answers: {row['answers']}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oergD3_BHGRY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}